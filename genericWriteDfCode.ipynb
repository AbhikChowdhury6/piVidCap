{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "cwd = os.getcwd()\n",
    "delimiter = \"\\\\\" if \"\\\\\" in cwd else \"/\"\n",
    "repoPath = delimiter.join(cwd.split(delimiter)[:cwd.split(delimiter).index(\"videoProcessing\")]) + delimiter\n",
    "\n",
    "workingDataPath = repoPath + \"workingData/\"\n",
    "recentCapturesPath = repoPath + \"recentCaptures/\"\n",
    "videoDataPath = repoPath + \"videoData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in a whole path to save so its more flexible\n",
    "# for the video data the path would be like\n",
    "# homeVideo/deskCam/frameMetaData/\n",
    "# homeVideo/deskCam/yolo11pose/\n",
    "# homeVideo/deskCam/yolo11Object/\n",
    "\n",
    "def getWorkingDf(fullWorkingDataPath):\n",
    "    workingDataFiles = os.listdir(fullWorkingDataPath)\n",
    "    if len(workingDataFiles) == 0:\n",
    "        print('no files found')\n",
    "        return []\n",
    "\n",
    "    dfSoFar = pd.read_parquet(workingDataHRPath + workingDataFiles[0])\n",
    "    for dataFileNameIndex in range(1, len(workingDataFiles)):\n",
    "        dfSoFar = pd.concat([dfSoFar, pd.read_parquet(workingDataHRPath + workingDataFiles[dataFileNameIndex])]) \n",
    "    dfSoFar = dfSoFar[~dfSoFar.index.duplicated(keep=\"first\")].sort_index()\n",
    "    return pd.DataFrame(dfSoFar['value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWorkingDf(fullWorkingDataPath):\n",
    "    workingDataFiles = os.listdir(fullWorkingDataPath)\n",
    "    if len(workingDataFiles) == 0:\n",
    "        print('no files found')\n",
    "        return []\n",
    "\n",
    "    #startTime\n",
    "    #endTime\n",
    "    numFilesAdded = 0\n",
    "    for wdf in workingDataFiles:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    # make a subset of files between start and end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute a short hash of a Python object\n",
    "def short_hash(obj, length=8):\n",
    "    # Serialize the object using pickle\n",
    "    obj_bytes = pickle.dumps(obj)\n",
    "    \n",
    "    # Compute MD5 hash of the serialized object\n",
    "    hash_obj = hashlib.md5(obj_bytes)\n",
    "    \n",
    "    # Return the hash truncated to the specified length\n",
    "    return hash_obj.hexdigest()[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this writes a file for a subset of a DF\n",
    "def writeDfFile(Df, fullWorkingDataPath):\n",
    "    sh = short_hash(Df)\n",
    "    parquetName = Df.iloc[0].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "                \"_\" +\\\n",
    "                Df.iloc[-1].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "                \"_\" + sh + \"_\" + \".parquet.gzip\"\n",
    "    print(f\"saved to a file named {parquetName}\")\n",
    "\n",
    "    Df.to_parquet(fullWorkingDataPath + parquetName,\n",
    "            compression='gzip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a dataframe you want to save and saves it in multiple files\n",
    "def saveRows(df, fullWorkingDataPath, rows_per_file):\n",
    "    if len(df) == 0: return\n",
    "    startRow = 0\n",
    "    endRow = len(df)\n",
    "    rows_remaining = endRow - startRow\n",
    "    while rows_remaining > 2 * rows_per_file:\n",
    "        print(f'{rows_remaining} is too many rows writing {startRow} to {(endRow - rows_remaining) + rows_per_file}')\n",
    "        writeDfFile(df.iloc[startRow: (endRow - rows_remaining) + rows_per_file + 1], fullWorkingDataPath)\n",
    "        rows_remaining -= rows_per_file\n",
    "        startRow += rows_per_file\n",
    "    writeDfFile(df.iloc[startRow:endRow+1], fullWorkingDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given dataframe approximates the number of rows for a parquet of target file size\n",
    "def rowsPerFile(Df, targetFileSize, fullWorkingDataPath, fileName = 'test.parquet.gzip'):\n",
    "    if fileName == 'test.parquet.gzip':\n",
    "        fileRows = 1_000_000\n",
    "        if len(Df) < fileRows: fileRows = len(Df)-1\n",
    "        Df.iloc[:fileRows].to_parquet(fullWorkingDataPath + fileName,\n",
    "                        compression='gzip')\n",
    "        file_size = os.path.getsize(fullWorkingDataPath + fileName)\n",
    "        os.remove(fullWorkingDataPath + fileName)\n",
    "    else:\n",
    "        fileRows = len(pd.read_parquet(fullWorkingDataPath + fileName))\n",
    "        file_size = os.path.getsize(fullWorkingDataPath + fileName)\n",
    "    \n",
    "    rows_per_file = int(fileRows//(file_size/targetFileSize))\n",
    "    return rows_per_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the filenames to split the rows in the df and saves\n",
    "# will check if no new rows were added to a file by comparing hashes\n",
    "# and skip save  \n",
    "\n",
    "def writeToExistingFiles(Df, fileNames, fullWorkingDataPath, rows_per_file):\n",
    "    tzi = Df.index[0].tzinfo\n",
    "    for fileNum, fileName in enumerate(fileNames):\n",
    "        if fileNum == 0 and Df.index[0] < pd.to_datetime(fileName.split('_')[0]).tz_convert(tzi):\n",
    "            startTime = Df.index[0]\n",
    "        else:\n",
    "            startTime = pd.to_datetime(fileName.split('_')[0]).tz_convert(tzi)\n",
    "        \n",
    "        if len(fileNames) == 1 or fileNum == len(fileNames) - 1:\n",
    "            endTime = Df.index[-1]\n",
    "        else:\n",
    "            endTime = pd.to_datetime(fileNames[fileNum + 1].split('_')[0]).tz_convert(tzi)\n",
    "        \n",
    "        # if the hash doesn't match write a new file\n",
    "        if short_hash(Df.loc[startTime:endTime]) != fileName.split('_')[2]:\n",
    "            print(\"the hashes don't match\")\n",
    "            os.remove(fullWorkingDataPath + fileName)\n",
    "            saveRows(Df.loc[startTime:endTime], fullWorkingDataPath, rows_per_file)\n",
    "        else:\n",
    "            print(f'hashes match for {fileName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves a df using existing filenames if available\n",
    "def writeWorkingDfParquet(location, Df, targetFileSize = 2 * 1024 * 1024):\n",
    "    fullWorkingDataPath = workingDataPath + location\n",
    "    fileNames = sorted(os.listdir(fullWorkingDataPath))\n",
    "\n",
    "    if len(fileNames) == 0:\n",
    "        rows_per_file = rowsPerFile(Df, targetFileSize, fullWorkingDataPath)\n",
    "        saveRows(Df, fullWorkingDataPath, rows_per_file)\n",
    "\n",
    "    else:\n",
    "        rows_per_file = rowsPerFile(Df, targetFileSize, fullWorkingDataPath, fileNames[0])\n",
    "        writeToExistingFiles(Df, fileNames, fullWorkingDataPath, rows_per_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
