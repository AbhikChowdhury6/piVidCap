{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "import pickle\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "cwd = os.getcwd()\n",
    "delimiter = \"\\\\\" if \"\\\\\" in cwd else \"/\"\n",
    "repoPath = delimiter.join(cwd.split(delimiter)[:cwd.split(delimiter).index(\"videoProcessing\")]) + delimiter\n",
    "\n",
    "workingDataPath = repoPath + \"workingData/\"\n",
    "recentCapturesPath = repoPath + \"recentCaptures/\"\n",
    "videoDataPath = repoPath + \"videoData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in a whole path to save so its more flexible\n",
    "# for the video data the path would be like\n",
    "# homeVideo/deskCam/frameMetaData/\n",
    "# homeVideo/deskCam/yolo11pose/\n",
    "# homeVideo/deskCam/yolo11Object/\n",
    "\n",
    "def getWorkingDf(fullWorkingDataPath):\n",
    "    workingDataFiles = os.listdir(fullWorkingDataPath)\n",
    "    if len(workingDataFiles) == 0:\n",
    "        print('no files found')\n",
    "        return []\n",
    "\n",
    "    dfSoFar = pd.read_parquet(workingDataHRPath + workingDataFiles[0])\n",
    "    for dataFileNameIndex in range(1, len(workingDataFiles)):\n",
    "        dfSoFar = pd.concat([dfSoFar, pd.read_parquet(workingDataHRPath + workingDataFiles[dataFileNameIndex])]) \n",
    "    dfSoFar = dfSoFar[~dfSoFar.index.duplicated(keep=\"first\")].sort_index()\n",
    "    return pd.DataFrame(dfSoFar['value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWorkingDf(location, interval = (pd.Timestamp.min.tz_localize(\"UTC\"), pd.Timestamp.max.tz_localize(\"UTC\"))):\n",
    "    fullWorkingDataPath = workingDataPath + location\n",
    "    workingDataFiles = os.listdir(fullWorkingDataPath)\n",
    "    if len(workingDataFiles) == 0:\n",
    "        print('no files found')\n",
    "        return []\n",
    "\n",
    "    startTime, endTime = interval\n",
    "    numFilesAdded = 0\n",
    "    relevantFiles = []\n",
    "    for wdf in workingDataFiles:\n",
    "        fileStartTime = pd.to_datetime(wdf.split(\"_\")[0])\n",
    "        fileEndTime = pd.to_datetime(wdf.split(\"_\")[1])\n",
    "        if fileEndTime > startTime and fileStartTime < endTime:\n",
    "            relevantFiles.append(wdf)\n",
    "    \n",
    "    if len(relevantFiles) == 0:\n",
    "        print('no relevant files found')\n",
    "        return []\n",
    "    \n",
    "    dfSoFar = pd.read_parquet(fullWorkingDataPath + relevantFiles[0])\n",
    "    for dataFileNameIndex in range(1, len(relevantFiles)):\n",
    "        dfSoFar = pd.concat([dfSoFar, pd.read_parquet(fullWorkingDataPath + relevantFiles[dataFileNameIndex])]) \n",
    "    dfSoFar = dfSoFar[~dfSoFar.index.duplicated(keep=\"first\")].sort_index()\n",
    "\n",
    "\n",
    "    return dfSoFar.loc[startTime:endTime].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute a short hash of a Python object\n",
    "def short_hash(obj, length=8):\n",
    "    # Serialize the object using pickle\n",
    "    obj_bytes = pickle.dumps(obj)\n",
    "    \n",
    "    # Compute MD5 hash of the serialized object\n",
    "    hash_obj = hashlib.md5(obj_bytes)\n",
    "    \n",
    "    # Return the hash truncated to the specified length\n",
    "    return hash_obj.hexdigest()[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this writes a file for a subset of a DF\n",
    "def writeDfFile(Df, fullWorkingDataPath):\n",
    "    sh = short_hash(Df)\n",
    "    parquetName = Df.iloc[0].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "                \"_\" +\\\n",
    "                Df.iloc[-1].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "                \"_\" + sh + \"_\" + \".parquet.gzip\"\n",
    "    print(f\"saved to a file named {parquetName}\")\n",
    "    print(f\"in {fullWorkingDataPath}\")\n",
    "\n",
    "    Df.to_parquet(fullWorkingDataPath + parquetName,\n",
    "            compression='gzip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a dataframe you want to save and saves it in multiple files\n",
    "def saveRows(df, fullWorkingDataPath, rows_per_file):\n",
    "    if len(df) == 0: return\n",
    "    startRow = 0\n",
    "    endRow = len(df)\n",
    "    rows_remaining = endRow - startRow\n",
    "    while rows_remaining > 2 * rows_per_file:\n",
    "        print(f'{rows_remaining} is too many rows writing {startRow} to {(endRow - rows_remaining) + rows_per_file}')\n",
    "        writeDfFile(df.iloc[startRow: (endRow - rows_remaining) + rows_per_file + 1], fullWorkingDataPath)\n",
    "        rows_remaining -= rows_per_file\n",
    "        startRow += rows_per_file\n",
    "    writeDfFile(df.iloc[startRow:endRow+1], fullWorkingDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given dataframe approximates the number of rows for a parquet of target file size\n",
    "def rowsPerFile(Df, targetFileSize, fullWorkingDataPath, fileName = 'test.parquet.gzip'):\n",
    "    if fileName == 'test.parquet.gzip':\n",
    "        fileRows = 1_000_000\n",
    "        if len(Df) < fileRows: fileRows = len(Df)-1\n",
    "        Df.iloc[:fileRows].to_parquet(fullWorkingDataPath + fileName,\n",
    "                        compression='gzip')\n",
    "        file_size = os.path.getsize(fullWorkingDataPath + fileName)\n",
    "        os.remove(fullWorkingDataPath + fileName)\n",
    "    else:\n",
    "        fileRows = len(pd.read_parquet(fullWorkingDataPath + fileName))\n",
    "        file_size = os.path.getsize(fullWorkingDataPath + fileName)\n",
    "    \n",
    "    rows_per_file = int(fileRows//(file_size/targetFileSize))\n",
    "    return rows_per_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the filenames to split the rows in the df and saves\n",
    "# will check if no new rows were added to a file by comparing hashes\n",
    "# and skip save  \n",
    "\n",
    "def writeToExistingFiles(Df, fileNames, fullWorkingDataPath, rows_per_file):\n",
    "    tzi = Df.index[0].tzinfo\n",
    "    for fileNum, fileName in enumerate(fileNames):\n",
    "        if fileNum == 0 and Df.index[0] < pd.to_datetime(fileName.split('_')[0]).tz_convert(tzi):\n",
    "            startTime = Df.index[0]\n",
    "        else:\n",
    "            startTime = pd.to_datetime(fileName.split('_')[0]).tz_convert(tzi)\n",
    "        \n",
    "        if len(fileNames) == 1 or fileNum == len(fileNames) - 1:\n",
    "            endTime = Df.index[-1]\n",
    "        else:\n",
    "            endTime = pd.to_datetime(fileNames[fileNum + 1].split('_')[0]).tz_convert(tzi)\n",
    "        \n",
    "        # if the hash doesn't match write a new file\n",
    "        if short_hash(Df.loc[startTime:endTime]) != fileName.split('_')[2]:\n",
    "            print(\"the hashes don't match\")\n",
    "            os.remove(fullWorkingDataPath + fileName)\n",
    "            saveRows(Df.loc[startTime:endTime], fullWorkingDataPath, rows_per_file)\n",
    "        else:\n",
    "            print(f'hashes match for {fileName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves a df using existing filenames if available\n",
    "def writeWorkingDf(location, Df, targetFileSize = 2 * 1024 * 1024):\n",
    "    fullWorkingDataPath = workingDataPath + location\n",
    "    if not os.path.exists(fullWorkingDataPath):\n",
    "        os.makedirs(fullWorkingDataPath)\n",
    "    fileNames = sorted(os.listdir(fullWorkingDataPath))\n",
    "\n",
    "    if len(fileNames) == 0:\n",
    "        rows_per_file = rowsPerFile(Df, targetFileSize, fullWorkingDataPath)\n",
    "        saveRows(Df, fullWorkingDataPath, rows_per_file)\n",
    "\n",
    "    else:\n",
    "        rows_per_file = rowsPerFile(Df, targetFileSize, fullWorkingDataPath, fileNames[0])\n",
    "        writeToExistingFiles(Df, fileNames, fullWorkingDataPath, rows_per_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sampleDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-14 18:19:15.001407-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-14 18:19:15.100091-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-14 18:19:15.200086-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-14 18:19:15.300096-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-14 18:19:15.400095-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>2024-11-14 21:27:15.000141-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>2024-11-14 21:27:30.000141-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>2024-11-14 21:27:45.000133-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>2024-11-14 21:28:00.000133-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>2024-11-14 21:28:15.000251-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             sampleDT\n",
       "0    2024-11-14 18:19:15.001407-07:00\n",
       "1    2024-11-14 18:19:15.100091-07:00\n",
       "2    2024-11-14 18:19:15.200086-07:00\n",
       "3    2024-11-14 18:19:15.300096-07:00\n",
       "4    2024-11-14 18:19:15.400095-07:00\n",
       "...                               ...\n",
       "1795 2024-11-14 21:27:15.000141-07:00\n",
       "1796 2024-11-14 21:27:30.000141-07:00\n",
       "1797 2024-11-14 21:27:45.000133-07:00\n",
       "1798 2024-11-14 21:28:00.000133-07:00\n",
       "1799 2024-11-14 21:28:15.000251-07:00\n",
       "\n",
       "[1800 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testParquetName = \"testvidMetaDataDf_2024-11-14T181915-001407-0700_2024-11-14T212815-000251-0700.parquet\"\n",
    "testLocation =  \"homeVideo/\" + testParquetName.split(\"_\")[0] + \"/frameMetaData/\"\n",
    "testParquet = pd.read_parquet(testParquetName)\n",
    "\n",
    "testParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testParquet = testParquet.set_index(\"sampleDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sampleDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-14 18:19:15.001407-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 18:19:15.100091-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 18:19:15.200086-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 18:19:15.300096-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 18:19:15.400095-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 21:27:15.000141-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 21:27:30.000141-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 21:27:45.000133-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 21:28:00.000133-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 21:28:15.000251-07:00</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [2024-11-14 18:19:15.001407-07:00, 2024-11-14 18:19:15.100091-07:00, 2024-11-14 18:19:15.200086-07:00, 2024-11-14 18:19:15.300096-07:00, 2024-11-14 18:19:15.400095-07:00, 2024-11-14 18:19:15.500095-07:00, 2024-11-14 18:19:15.600094-07:00, 2024-11-14 18:19:15.700095-07:00, 2024-11-14 18:19:15.800095-07:00, 2024-11-14 18:19:15.900134-07:00, 2024-11-14 18:19:16.000103-07:00, 2024-11-14 18:19:16.100093-07:00, 2024-11-14 18:19:16.200096-07:00, 2024-11-14 18:19:16.300095-07:00, 2024-11-14 18:19:16.400096-07:00, 2024-11-14 18:19:16.500095-07:00, 2024-11-14 18:19:16.600095-07:00, 2024-11-14 18:19:16.700095-07:00, 2024-11-14 18:19:16.800093-07:00, 2024-11-14 18:19:16.900091-07:00, 2024-11-14 18:19:17.000091-07:00, 2024-11-14 18:19:17.100091-07:00, 2024-11-14 18:19:17.200098-07:00, 2024-11-14 18:19:17.300104-07:00, 2024-11-14 18:19:17.400095-07:00, 2024-11-14 18:19:17.500095-07:00, 2024-11-14 18:19:17.600095-07:00, 2024-11-14 18:19:17.700097-07:00, 2024-11-14 18:19:17.800095-07:00, 2024-11-14 18:19:17.900135-07:00, 2024-11-14 18:19:18.000099-07:00, 2024-11-14 18:19:18.100096-07:00, 2024-11-14 18:19:18.200093-07:00, 2024-11-14 18:19:18.300095-07:00, 2024-11-14 18:19:18.400094-07:00, 2024-11-14 18:19:18.500093-07:00, 2024-11-14 18:19:18.600095-07:00, 2024-11-14 18:19:18.700095-07:00, 2024-11-14 18:19:18.800095-07:00, 2024-11-14 18:19:18.900135-07:00, 2024-11-14 18:19:19.000100-07:00, 2024-11-14 18:19:19.100096-07:00, 2024-11-14 18:19:19.200132-07:00, 2024-11-14 18:19:19.300103-07:00, 2024-11-14 18:19:19.400097-07:00, 2024-11-14 18:19:19.500135-07:00, 2024-11-14 18:19:19.600102-07:00, 2024-11-14 18:19:19.700095-07:00, 2024-11-14 18:19:19.800096-07:00, 2024-11-14 18:19:19.900136-07:00, 2024-11-14 18:19:20.000102-07:00, 2024-11-14 18:19:20.100093-07:00, 2024-11-14 18:19:20.200133-07:00, 2024-11-14 18:19:20.300100-07:00, 2024-11-14 18:19:20.400093-07:00, 2024-11-14 18:19:20.500133-07:00, 2024-11-14 18:19:20.600100-07:00, 2024-11-14 18:19:20.700095-07:00, 2024-11-14 18:19:20.800096-07:00, 2024-11-14 18:19:20.900135-07:00, 2024-11-14 18:19:21.000101-07:00, 2024-11-14 18:19:21.100097-07:00, 2024-11-14 18:19:21.200136-07:00, 2024-11-14 18:19:21.300103-07:00, 2024-11-14 18:19:21.400095-07:00, 2024-11-14 18:19:21.500095-07:00, 2024-11-14 18:19:21.600095-07:00, 2024-11-14 18:19:21.700095-07:00, 2024-11-14 18:19:21.800095-07:00, 2024-11-14 18:19:21.900095-07:00, 2024-11-14 18:19:22.000095-07:00, 2024-11-14 18:19:22.100134-07:00, 2024-11-14 18:19:22.200099-07:00, 2024-11-14 18:19:22.300135-07:00, 2024-11-14 18:19:22.400102-07:00, 2024-11-14 18:19:22.500092-07:00, 2024-11-14 18:19:22.600094-07:00, 2024-11-14 18:19:22.700095-07:00, 2024-11-14 18:19:22.800097-07:00, 2024-11-14 18:19:22.900092-07:00, 2024-11-14 18:19:23.000091-07:00, 2024-11-14 18:19:23.100095-07:00, 2024-11-14 18:19:23.200132-07:00, 2024-11-14 18:19:23.300142-07:00, 2024-11-14 18:19:23.400100-07:00, 2024-11-14 18:19:23.500093-07:00, 2024-11-14 18:19:23.600095-07:00, 2024-11-14 18:19:23.700095-07:00, 2024-11-14 18:19:23.800095-07:00, 2024-11-14 18:19:23.900095-07:00, 2024-11-14 18:19:24.000095-07:00, 2024-11-14 18:19:24.100094-07:00, 2024-11-14 18:19:24.200136-07:00, 2024-11-14 18:19:24.300103-07:00, 2024-11-14 18:19:24.400095-07:00, 2024-11-14 18:19:24.500096-07:00, 2024-11-14 18:19:24.600095-07:00, 2024-11-14 18:19:24.700094-07:00, 2024-11-14 18:19:24.800095-07:00, 2024-11-14 18:19:24.900093-07:00, ...]\n",
       "\n",
       "[1800 rows x 0 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to a file named 2024-11-14T181915-0700_2024-11-14T212815-0700_fc46e221_.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "writeWorkingDf(testLocation, testParquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sampleDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-14 18:19:15.001407-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 18:19:15.100091-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 18:19:15.200086-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 18:19:15.300096-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 18:19:15.400095-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 21:27:15.000141-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 21:27:30.000141-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 21:27:45.000133-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 21:28:00.000133-07:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 21:28:15.000251-07:00</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [2024-11-14 18:19:15.001407-07:00, 2024-11-14 18:19:15.100091-07:00, 2024-11-14 18:19:15.200086-07:00, 2024-11-14 18:19:15.300096-07:00, 2024-11-14 18:19:15.400095-07:00, 2024-11-14 18:19:15.500095-07:00, 2024-11-14 18:19:15.600094-07:00, 2024-11-14 18:19:15.700095-07:00, 2024-11-14 18:19:15.800095-07:00, 2024-11-14 18:19:15.900134-07:00, 2024-11-14 18:19:16.000103-07:00, 2024-11-14 18:19:16.100093-07:00, 2024-11-14 18:19:16.200096-07:00, 2024-11-14 18:19:16.300095-07:00, 2024-11-14 18:19:16.400096-07:00, 2024-11-14 18:19:16.500095-07:00, 2024-11-14 18:19:16.600095-07:00, 2024-11-14 18:19:16.700095-07:00, 2024-11-14 18:19:16.800093-07:00, 2024-11-14 18:19:16.900091-07:00, 2024-11-14 18:19:17.000091-07:00, 2024-11-14 18:19:17.100091-07:00, 2024-11-14 18:19:17.200098-07:00, 2024-11-14 18:19:17.300104-07:00, 2024-11-14 18:19:17.400095-07:00, 2024-11-14 18:19:17.500095-07:00, 2024-11-14 18:19:17.600095-07:00, 2024-11-14 18:19:17.700097-07:00, 2024-11-14 18:19:17.800095-07:00, 2024-11-14 18:19:17.900135-07:00, 2024-11-14 18:19:18.000099-07:00, 2024-11-14 18:19:18.100096-07:00, 2024-11-14 18:19:18.200093-07:00, 2024-11-14 18:19:18.300095-07:00, 2024-11-14 18:19:18.400094-07:00, 2024-11-14 18:19:18.500093-07:00, 2024-11-14 18:19:18.600095-07:00, 2024-11-14 18:19:18.700095-07:00, 2024-11-14 18:19:18.800095-07:00, 2024-11-14 18:19:18.900135-07:00, 2024-11-14 18:19:19.000100-07:00, 2024-11-14 18:19:19.100096-07:00, 2024-11-14 18:19:19.200132-07:00, 2024-11-14 18:19:19.300103-07:00, 2024-11-14 18:19:19.400097-07:00, 2024-11-14 18:19:19.500135-07:00, 2024-11-14 18:19:19.600102-07:00, 2024-11-14 18:19:19.700095-07:00, 2024-11-14 18:19:19.800096-07:00, 2024-11-14 18:19:19.900136-07:00, 2024-11-14 18:19:20.000102-07:00, 2024-11-14 18:19:20.100093-07:00, 2024-11-14 18:19:20.200133-07:00, 2024-11-14 18:19:20.300100-07:00, 2024-11-14 18:19:20.400093-07:00, 2024-11-14 18:19:20.500133-07:00, 2024-11-14 18:19:20.600100-07:00, 2024-11-14 18:19:20.700095-07:00, 2024-11-14 18:19:20.800096-07:00, 2024-11-14 18:19:20.900135-07:00, 2024-11-14 18:19:21.000101-07:00, 2024-11-14 18:19:21.100097-07:00, 2024-11-14 18:19:21.200136-07:00, 2024-11-14 18:19:21.300103-07:00, 2024-11-14 18:19:21.400095-07:00, 2024-11-14 18:19:21.500095-07:00, 2024-11-14 18:19:21.600095-07:00, 2024-11-14 18:19:21.700095-07:00, 2024-11-14 18:19:21.800095-07:00, 2024-11-14 18:19:21.900095-07:00, 2024-11-14 18:19:22.000095-07:00, 2024-11-14 18:19:22.100134-07:00, 2024-11-14 18:19:22.200099-07:00, 2024-11-14 18:19:22.300135-07:00, 2024-11-14 18:19:22.400102-07:00, 2024-11-14 18:19:22.500092-07:00, 2024-11-14 18:19:22.600094-07:00, 2024-11-14 18:19:22.700095-07:00, 2024-11-14 18:19:22.800097-07:00, 2024-11-14 18:19:22.900092-07:00, 2024-11-14 18:19:23.000091-07:00, 2024-11-14 18:19:23.100095-07:00, 2024-11-14 18:19:23.200132-07:00, 2024-11-14 18:19:23.300142-07:00, 2024-11-14 18:19:23.400100-07:00, 2024-11-14 18:19:23.500093-07:00, 2024-11-14 18:19:23.600095-07:00, 2024-11-14 18:19:23.700095-07:00, 2024-11-14 18:19:23.800095-07:00, 2024-11-14 18:19:23.900095-07:00, 2024-11-14 18:19:24.000095-07:00, 2024-11-14 18:19:24.100094-07:00, 2024-11-14 18:19:24.200136-07:00, 2024-11-14 18:19:24.300103-07:00, 2024-11-14 18:19:24.400095-07:00, 2024-11-14 18:19:24.500096-07:00, 2024-11-14 18:19:24.600095-07:00, 2024-11-14 18:19:24.700094-07:00, 2024-11-14 18:19:24.800095-07:00, 2024-11-14 18:19:24.900093-07:00, ...]\n",
       "\n",
       "[1800 rows x 0 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = getWorkingDf(testLocation)\n",
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
